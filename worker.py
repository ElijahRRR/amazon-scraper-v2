"""
Amazon äº§å“é‡‡é›†ç³»ç»Ÿ v2 - Worker é‡‡é›†å¼•æ“
è¿æ¥ä¸­å¤®æœåŠ¡å™¨ API æ‹‰å–ä»»åŠ¡ã€æ¨é€ç»“æœ
æ¯ä¸ª worker ç»´æŠ¤ç‹¬ç«‹ session
ä¸¥æ ¼ 5æ¬¡/s é™é€Ÿï¼ˆ200ms Â± 50ms éšæœºæŠ–åŠ¨ï¼‰
è¢«å°æ£€æµ‹ â†’ æ¢ IP + æ¢ session é‡è¯•ï¼Œæœ€å¤š 3 æ¬¡
å¯åŠ¨æ–¹å¼ï¼špython worker.py --server http://x.x.x.x:8899
"""
import asyncio
import argparse
import logging
import random
import time
import uuid
import signal
import sys
from typing import Optional, Dict, List

from curl_cffi import requests as curl_requests

import config
from proxy import ProxyManager, get_proxy_manager
from session import AmazonSession
from parser import AmazonParser

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger(__name__)


class Worker:
    """å¼‚æ­¥é‡‡é›† Worker"""

    def __init__(self, server_url: str, worker_id: str = None, concurrency: int = None,
                 zip_code: str = None):
        self.server_url = server_url.rstrip("/")
        self.worker_id = worker_id or f"worker-{uuid.uuid4().hex[:8]}"
        self.concurrency = concurrency or config.DEFAULT_CONCURRENCY
        self.zip_code = zip_code or config.DEFAULT_ZIP_CODE

        # ç»„ä»¶
        self.proxy_manager = get_proxy_manager()
        self.parser = AmazonParser()
        self._session: Optional[AmazonSession] = None

        # é€Ÿç‡æ§åˆ¶
        self._interval = config.REQUEST_INTERVAL
        self._jitter = config.REQUEST_JITTER
        self._semaphore = asyncio.Semaphore(self.concurrency)

        # ç»Ÿè®¡
        self._stats = {
            "total": 0,
            "success": 0,
            "failed": 0,
            "blocked": 0,
            "start_time": None,
        }

        # è¿è¡Œæ§åˆ¶
        self._running = False

    async def start(self):
        """å¯åŠ¨ Worker"""
        logger.info(f"ğŸš€ Worker [{self.worker_id}] å¯åŠ¨")
        logger.info(f"   æœåŠ¡å™¨: {self.server_url}")
        logger.info(f"   å¹¶å‘æ•°: {self.concurrency}")
        logger.info(f"   é‚®ç¼–: {self.zip_code}")

        self._running = True
        self._stats["start_time"] = time.time()

        # åˆå§‹åŒ– session
        await self._init_session()

        # ä¸»å¾ªç¯ï¼šæŒç»­æ‹‰å–å’Œå¤„ç†ä»»åŠ¡
        while self._running:
            try:
                tasks = await self._pull_tasks()
                if not tasks:
                    logger.info("ğŸ“­ æš‚æ— ä»»åŠ¡ï¼Œç­‰å¾… 5 ç§’...")
                    await asyncio.sleep(5)
                    continue

                logger.info(f"ğŸ“‹ æ‹‰å–åˆ° {len(tasks)} ä¸ªä»»åŠ¡")

                # å¹¶å‘å¤„ç†ä»»åŠ¡
                sem_tasks = [self._process_with_semaphore(task) for task in tasks]
                await asyncio.gather(*sem_tasks, return_exceptions=True)

            except KeyboardInterrupt:
                break
            except Exception as e:
                logger.error(f"âŒ ä¸»å¾ªç¯å¼‚å¸¸: {e}")
                await asyncio.sleep(3)

        await self._cleanup()
        logger.info(f"ğŸ›‘ Worker [{self.worker_id}] å·²åœæ­¢")
        self._print_stats()

    async def stop(self):
        """åœæ­¢ Worker"""
        self._running = False

    async def _init_session(self):
        """åˆå§‹åŒ– Amazon session"""
        logger.info("ğŸ”§ åˆå§‹åŒ– Amazon session...")
        self._session = AmazonSession(self.proxy_manager, self.zip_code)
        success = await self._session.initialize()
        if not success:
            logger.warning("âš ï¸ Session åˆå§‹åŒ–å¤±è´¥ï¼Œå°†åœ¨é¦–æ¬¡è¯·æ±‚æ—¶é‡è¯•")

    async def _pull_tasks(self) -> List[Dict]:
        """ä»æœåŠ¡å™¨æ‹‰å–ä»»åŠ¡"""
        try:
            url = f"{self.server_url}/api/tasks/pull"
            params = {
                "worker_id": self.worker_id,
                "count": self.concurrency,
            }
            resp = curl_requests.get(url, params=params, timeout=10)
            if resp.status_code == 200:
                data = resp.json()
                return data.get("tasks", [])
            else:
                logger.warning(f"æ‹‰å–ä»»åŠ¡å¤±è´¥: HTTP {resp.status_code}")
                return []
        except Exception as e:
            logger.error(f"æ‹‰å–ä»»åŠ¡å¼‚å¸¸: {e}")
            return []

    async def _process_with_semaphore(self, task: Dict):
        """å¸¦ä¿¡å·é‡çš„ä»»åŠ¡å¤„ç†ï¼ˆæ§åˆ¶å¹¶å‘ï¼‰"""
        async with self._semaphore:
            await self._process_task(task)

    async def _process_task(self, task: Dict):
        """
        å¤„ç†å•ä¸ªé‡‡é›†ä»»åŠ¡
        åŒºåˆ†è¶…æ—¶å’ŒçœŸæ­£çš„å°é”ï¼š
        - è¶…æ—¶/ç½‘ç»œé”™è¯¯ â†’ ç­‰å¾…åç›´æ¥é‡è¯•ï¼ˆä¸æ¢ IPï¼‰
        - éªŒè¯ç /403/503 â†’ æ¢ IP + æ¢ session â†’ é‡è¯•
        """
        asin = task["asin"]
        task_id = task["id"]
        zip_code = task.get("zip_code", self.zip_code)
        max_retries = config.MAX_RETRIES

        for attempt in range(max_retries):
            try:
                # é€Ÿç‡æ§åˆ¶ï¼š200ms Â± 50ms éšæœºæŠ–åŠ¨
                delay = self._interval + random.uniform(-self._jitter, self._jitter)
                await asyncio.sleep(delay)

                # å‘èµ·è¯·æ±‚
                resp = await self._session.fetch_product_page(asin)

                # è¯·æ±‚å¤±è´¥ï¼ˆè¶…æ—¶/ç½‘ç»œå¼‚å¸¸ï¼‰â†’ ä¸æ¢ IPï¼Œç­‰å¾…åé‡è¯•
                if resp is None:
                    logger.warning(f"ASIN {asin} è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt+1}/{max_retries})")
                    await asyncio.sleep(2)
                    continue

                # çœŸæ­£è¢«å°ï¼ˆ403/503/éªŒè¯ç ï¼‰â†’ æ¢ IP + æ¢ session
                if self._session.is_blocked(resp):
                    self._stats["blocked"] += 1
                    logger.warning(f"ASIN {asin} è¢«å° HTTP {resp.status_code} (å°è¯• {attempt+1}/{max_retries})")
                    await self.proxy_manager.report_blocked()
                    await self._session.close()
                    await self._init_session()
                    continue

                # 404 å¤„ç†
                if self._session.is_404(resp):
                    logger.info(f"ASIN {asin} å•†å“ä¸å­˜åœ¨ (404)")
                    result_data = self.parser._default_result(asin, zip_code)
                    result_data["title"] = "[å•†å“ä¸å­˜åœ¨]"
                    result_data["batch_name"] = task.get("batch_name", "")
                    await self._submit_result(task_id, result_data, success=True)
                    self._stats["success"] += 1
                    self._stats["total"] += 1
                    return

                # è§£æé¡µé¢
                result_data = self.parser.parse_product(resp.text, asin, zip_code)
                result_data["batch_name"] = task.get("batch_name", "")

                # æ£€æŸ¥æ˜¯å¦æ˜¯æ‹¦æˆªé¡µé¢
                if result_data["title"] in ["[éªŒè¯ç æ‹¦æˆª]", "[APIå°é”]"]:
                    self._stats["blocked"] += 1
                    logger.warning(f"ASIN {asin} {result_data['title']} (å°è¯• {attempt+1}/{max_retries})")
                    await self.proxy_manager.report_blocked()
                    await self._session.close()
                    await self._init_session()
                    continue

                # æ ‡é¢˜ä¸ºç©ºè§†ä¸ºè½¯æ‹¦æˆª
                if not result_data["title"] or result_data["title"] == "N/A":
                    logger.warning(f"ASIN {asin} æ ‡é¢˜ä¸ºç©º (å°è¯• {attempt+1}/{max_retries})")
                    if attempt < max_retries - 1:
                        await asyncio.sleep(2)
                        continue

                # æˆåŠŸ
                await self._submit_result(task_id, result_data, success=True)
                self._stats["success"] += 1
                self._stats["total"] += 1

                title_short = result_data["title"][:40] if result_data["title"] else "N/A"
                logger.info(f"OK {asin} | {title_short}... | {result_data['current_price']}")
                return

            except Exception as e:
                logger.error(f"ASIN {asin} å¼‚å¸¸ (å°è¯• {attempt+1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(2)
                    continue

        # æ‰€æœ‰é‡è¯•ç”¨å®Œï¼Œæ ‡è®°å¤±è´¥
        logger.error(f"ASIN {asin} é‡‡é›†å¤±è´¥ (å·²é‡è¯• {max_retries} æ¬¡)")
        await self._submit_result(task_id, None, success=False)
        self._stats["failed"] += 1
        self._stats["total"] += 1

    async def _submit_result(self, task_id: int, result_data: Optional[Dict], success: bool):
        """æäº¤é‡‡é›†ç»“æœåˆ°æœåŠ¡å™¨"""
        try:
            url = f"{self.server_url}/api/tasks/result"
            payload = {
                "task_id": task_id,
                "worker_id": self.worker_id,
                "success": success,
                "result": result_data,
            }
            resp = curl_requests.post(
                url,
                json=payload,
                timeout=10,
            )
            if resp.status_code != 200:
                logger.warning(f"æäº¤ç»“æœå¤±è´¥: HTTP {resp.status_code}")
        except Exception as e:
            logger.error(f"æäº¤ç»“æœå¼‚å¸¸: {e}")

    async def _cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self._session:
            await self._session.close()

    def _print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        elapsed = time.time() - self._stats["start_time"] if self._stats["start_time"] else 0
        total = self._stats["total"]
        success = self._stats["success"]
        rate = success / total * 100 if total > 0 else 0
        speed = total / elapsed * 60 if elapsed > 0 else 0
        
        logger.info("=" * 50)
        logger.info(f"ğŸ“Š Worker [{self.worker_id}] ç»Ÿè®¡")
        logger.info(f"   æ€»é‡‡é›†: {total}")
        logger.info(f"   æˆåŠŸ: {success} ({rate:.1f}%)")
        logger.info(f"   å¤±è´¥: {self._stats['failed']}")
        logger.info(f"   è¢«å°: {self._stats['blocked']}")
        logger.info(f"   é€Ÿåº¦: {speed:.1f} æ¡/åˆ†é’Ÿ")
        logger.info(f"   è€—æ—¶: {elapsed:.0f} ç§’")
        logger.info("=" * 50)


def main():
    """Worker å…¥å£"""
    arg_parser = argparse.ArgumentParser(description="Amazon Scraper Worker")
    arg_parser.add_argument("--server", required=True, help="ä¸­å¤®æœåŠ¡å™¨åœ°å€ (å¦‚ http://192.168.1.100:8899)")
    arg_parser.add_argument("--worker-id", default=None, help="Worker IDï¼ˆé»˜è®¤è‡ªåŠ¨ç”Ÿæˆï¼‰")
    arg_parser.add_argument("--concurrency", type=int, default=None, help=f"å¹¶å‘æ•°ï¼ˆé»˜è®¤ {config.DEFAULT_CONCURRENCY}ï¼‰")
    arg_parser.add_argument("--zip-code", default=None, help=f"é‚®ç¼–ï¼ˆé»˜è®¤ {config.DEFAULT_ZIP_CODE}ï¼‰")
    
    args = arg_parser.parse_args()

    worker = Worker(
        server_url=args.server,
        worker_id=args.worker_id,
        concurrency=args.concurrency,
        zip_code=args.zip_code,
    )

    # ä¼˜é›…é€€å‡º
    loop = asyncio.new_event_loop()
    
    def signal_handler(sig, frame):
        logger.info("â¹ï¸ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...")
        loop.create_task(worker.stop())
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    try:
        loop.run_until_complete(worker.start())
    finally:
        loop.close()


if __name__ == "__main__":
    main()
