"""
Amazon äº§å“é‡‡é›†ç³»ç»Ÿ v2 - ä¸­å¤®æœåŠ¡å™¨ï¼ˆFastAPIï¼‰
ç«¯å£ 8899
æä¾› API ç«¯ç‚¹å’Œ Web UI
"""
import os
import io
import csv
import random
import zipfile
import re
import asyncio
import logging
import time
from contextlib import asynccontextmanager
from datetime import datetime
from typing import Optional, List, Dict, Any

from fastapi import FastAPI, Request, UploadFile, File, Form, Query, HTTPException
from fastapi.responses import HTMLResponse, JSONResponse, StreamingResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
import uvicorn
import openpyxl

import config
from database import Database, get_db, close_db
from models import RESULT_FIELDS

# æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger(__name__)

# ==================== ç”Ÿå‘½å‘¨æœŸ ====================

@asynccontextmanager
async def lifespan(app):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # startup
    db = await get_db()
    logger.info("âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
    asyncio.create_task(_timeout_task_loop())
    yield
    # shutdown
    await close_db()
    logger.info("ğŸ›‘ æœåŠ¡å™¨å…³é—­")


# FastAPI åº”ç”¨
app = FastAPI(title="Amazon Scraper v2", version="2.0.0", lifespan=lifespan)

# é™æ€æ–‡ä»¶å’Œæ¨¡æ¿
app.mount("/static", StaticFiles(directory=config.STATIC_DIR), name="static")
templates = Jinja2Templates(directory=config.TEMPLATE_DIR)

# ==================== Worker åœ¨çº¿çŠ¶æ€è¿½è¸ª ====================
# å­˜å‚¨ worker çš„æœ€åå¿ƒè·³æ—¶é—´å’Œç»Ÿè®¡
_worker_registry: Dict[str, Dict] = {}


def _register_worker(worker_id: str):
    """æ³¨å†Œ/æ›´æ–° worker å¿ƒè·³"""
    now = time.time()
    if worker_id not in _worker_registry:
        _worker_registry[worker_id] = {
            "worker_id": worker_id,
            "first_seen": now,
            "last_seen": now,
            "tasks_pulled": 0,
            "results_submitted": 0,
        }
    _worker_registry[worker_id]["last_seen"] = now


# ==================== è¿è¡Œæ—¶è®¾ç½®ï¼ˆå¯é€šè¿‡ API ä¿®æ”¹ï¼ŒWorker å®šæœŸåŒæ­¥ï¼‰====================
_runtime_settings = {
    # åŸºç¡€
    "zip_code": config.DEFAULT_ZIP_CODE,
    "max_retries": config.MAX_RETRIES,
    "proxy_api_url": config.PROXY_API_URL_AUTH,
    # é™é€Ÿ
    "token_bucket_rate": config.TOKEN_BUCKET_RATE,
    # å¹¶å‘æ§åˆ¶
    "initial_concurrency": config.INITIAL_CONCURRENCY,
    "min_concurrency": config.MIN_CONCURRENCY,
    "max_concurrency": config.MAX_CONCURRENCY,
    # Session
    "session_rotate_every": config.SESSION_ROTATE_EVERY,
    # æˆªå›¾
    "screenshot_concurrency": 3,
    # AIMD è°ƒæ§
    "adjust_interval": config.ADJUST_INTERVAL_S,
    "target_latency": config.TARGET_LATENCY_S,
    "max_latency": config.MAX_LATENCY_S,
    "target_success_rate": config.TARGET_SUCCESS_RATE,
    "min_success_rate": config.MIN_SUCCESS_RATE,
    "block_rate_threshold": config.BLOCK_RATE_THRESHOLD,
    "cooldown_after_block": config.COOLDOWN_AFTER_BLOCK_S,
    # å…¨å±€å¹¶å‘åè°ƒ
    "global_max_concurrency": config.GLOBAL_MAX_CONCURRENCY,
    "global_max_qps": config.GLOBAL_MAX_QPS,
}
# è®¾ç½®ç‰ˆæœ¬å·ï¼šæ¯æ¬¡ä¿®æ”¹ +1ï¼ŒWorker æ¯”å¯¹ç‰ˆæœ¬å·å†³å®šæ˜¯å¦éœ€è¦é‡è½½
_settings_version = 0

# ==================== å…¨å±€å¹¶å‘åè°ƒå™¨ ====================
_global_coordinator = {
    "worker_metrics": {},       # {worker_id: {snapshot å­—æ®µ + reported_at}}
    "worker_quotas": {},        # {worker_id: {concurrency, qps, assigned_at}}
    "global_block_until": 0.0,  # monotonic æ—¶é—´æˆ³ï¼›> now è¡¨ç¤ºå…¨å±€å†·å´ä¸­
    "block_count": 0,           # ç´¯è®¡å…¨å±€å°é”æ¬¡æ•°
    "recovery_epoch": 0,        # æ¯æ¬¡å…¨å±€å°é” +1ï¼ŒWorker ç”¨äºå»é‡
    "recovery_jitter": {},      # {worker_id: float 0.0~1.0} æ¢å¤æŠ–åŠ¨ç³»æ•°
}


def _allocate_quotas():
    """æŒ‰å¥åº·åº¦åŠ æƒåˆ†é…å…¨å±€å¹¶å‘/QPS é¢„ç®—ç»™æ´»è·ƒ Worker"""
    now = time.time()
    active = {wid: info for wid, info in _worker_registry.items()
              if now - info["last_seen"] < 60}
    n = len(active)
    if n == 0:
        _global_coordinator["worker_quotas"] = {}
        return

    g = _global_coordinator
    total_conc = _runtime_settings["global_max_concurrency"]
    total_qps = _runtime_settings["global_max_qps"]

    # å…¨å±€å†·å´æœŸå†…é¢„ç®—å‡åŠ
    now_mono = time.monotonic()
    in_cooldown = now_mono < g["global_block_until"]
    if in_cooldown:
        total_conc = max(n * _runtime_settings["min_concurrency"], total_conc // 2)
        total_qps = max(n * 0.5, total_qps / 2)

    # è®¡ç®—æ¯ä¸ª Worker çš„å¥åº·åˆ†ï¼ˆ0.1 ~ 1.0ï¼‰
    scores = {}
    for wid in active:
        metrics = g["worker_metrics"].get(wid, {})
        if not metrics or now - metrics.get("reported_at", 0) > 90:
            scores[wid] = 0.5  # æ— æ•°æ®æˆ–è¿‡æœŸ â†’ é»˜è®¤åˆ†
        else:
            sr = metrics.get("success_rate", 0.95)
            br = metrics.get("block_rate", 0.0)
            # å°é”ç‡æƒ©ç½šæƒé‡ Ã—5ï¼šblock_rate=10% â†’ åˆ†æ•°å‡åŠ
            score = sr * max(0.0, 1.0 - br * 5)
            scores[wid] = max(0.1, min(1.0, score))

    total_score = sum(scores.values())

    # ç¬¬ä¸€è½®ï¼šæŒ‰æƒé‡è®¡ç®—åŸå§‹é…é¢ï¼ˆä¸è®¾ä¸‹é™ï¼Œçº¯æ¯”ä¾‹åˆ†é…ï¼‰
    raw_conc = {}
    raw_qps = {}
    for wid in active:
        weight = scores[wid] / total_score
        raw_conc[wid] = total_conc * weight
        raw_qps[wid] = total_qps * weight

        # å†·å´æœŸå åŠ æŠ–åŠ¨ç³»æ•°
        if in_cooldown:
            jitter = g["recovery_jitter"].get(wid, 0.5)
            raw_conc[wid] *= (0.5 + 0.5 * jitter)
            raw_qps[wid] *= (0.5 + 0.5 * jitter)

    # ç¬¬äºŒè½®ï¼šå–æ•´ + ä¸‹é™ä¿æŠ¤ï¼Œç„¶åè£å‰ªæ€»é‡ä¸è¶…é¢„ç®—
    min_c = _runtime_settings["min_concurrency"]
    # å½“ min_c Ã— n > budget æ—¶ï¼Œé¢„ç®—ä¼˜å…ˆï¼šé™ä½æœ‰æ•ˆä¸‹é™
    effective_min_c = min(min_c, max(1, int(total_conc / n)))
    int_conc = {wid: max(effective_min_c, int(v)) for wid, v in raw_conc.items()}

    # å¦‚æœæ€»å’Œè¶…é¢„ç®—ï¼ŒæŒ‰æ¯”ä¾‹ç¼©å‡ï¼ˆä¿ç•™æœ‰æ•ˆä¸‹é™ï¼‰
    sum_conc = sum(int_conc.values())
    if sum_conc > total_conc:
        excess = sum_conc - total_conc
        reducible = {wid: v - effective_min_c for wid, v in int_conc.items() if v > effective_min_c}
        total_reducible = sum(reducible.values())
        if total_reducible > 0:
            for wid in reducible:
                cut = int(excess * reducible[wid] / total_reducible)
                int_conc[wid] = max(effective_min_c, int_conc[wid] - cut)
        # æœ€ç»ˆå…œåº•ï¼šé€ä¸ªå‰Šå‡ç›´åˆ°ä¸è¶…é¢„ç®—
        while sum(int_conc.values()) > total_conc:
            for wid in sorted(int_conc, key=lambda w: int_conc[w], reverse=True):
                if int_conc[wid] > effective_min_c and sum(int_conc.values()) > total_conc:
                    int_conc[wid] -= 1

    sum_qps = sum(raw_qps.values())
    if sum_qps > total_qps and sum_qps > 0:
        scale = total_qps / sum_qps
        raw_qps = {wid: v * scale for wid, v in raw_qps.items()}

    new_quotas = {}
    for wid in active:
        new_quotas[wid] = {
            "concurrency": int_conc[wid],
            "qps": round(max(0.1, raw_qps[wid]), 2),
            "assigned_at": now,
        }

    g["worker_quotas"] = new_quotas


def _handle_worker_metrics(worker_id: str, metrics: dict):
    """å¤„ç† Worker ä¸ŠæŠ¥çš„ metricsï¼Œå¿…è¦æ—¶è§¦å‘å…¨å±€å°é”"""
    g = _global_coordinator
    now_mono = time.monotonic()

    # å­˜å‚¨ metrics
    g["worker_metrics"][worker_id] = {
        **metrics,
        "reported_at": time.time(),
    }

    # æ£€æŸ¥æ˜¯å¦è§¦å‘å…¨å±€å°é”
    block_rate = metrics.get("block_rate", 0.0)
    threshold = _runtime_settings.get("block_rate_threshold", 0.05)

    if block_rate > threshold and now_mono >= g["global_block_until"]:
        cooldown = _runtime_settings.get("cooldown_after_block", 30)
        g["global_block_until"] = now_mono + cooldown
        g["block_triggered_by"] = worker_id
        g["block_count"] += 1
        g["recovery_epoch"] += 1

        # ä¸ºæ¯ä¸ªæ´»è·ƒ Worker åˆ†é…ä¸åŒçš„æ¢å¤æŠ–åŠ¨ç³»æ•°
        for wid in _worker_registry:
            g["recovery_jitter"][wid] = random.random()

        logger.warning(
            f"âš ï¸ å…¨å±€å°é”è§¦å‘ by {worker_id} "
            f"(block_rate={block_rate:.1%}), "
            f"å†·å´ {cooldown}s, epoch={g['recovery_epoch']}"
        )

        # ç«‹å³é‡æ–°åˆ†é…é…é¢ï¼ˆå‡åŠç”Ÿæ•ˆï¼‰
        _allocate_quotas()


async def _timeout_task_loop():
    """å®šæœŸå›é€€è¶…æ—¶ processing ä»»åŠ¡ï¼Œå¹¶æ¸…ç†é•¿æœŸç¦»çº¿ Worker"""
    while True:
        try:
            db = await get_db()
            count = await db.reset_timeout_tasks()
            if count > 0:
                logger.info(f"å›é€€äº† {count} ä¸ªè¶…æ—¶ä»»åŠ¡")
        except Exception as e:
            logger.error(f"è¶…æ—¶ä»»åŠ¡å›é€€å¼‚å¸¸: {e}")

        # æ¸…ç†è¶…è¿‡ 10 åˆ†é’Ÿæ— å¿ƒè·³çš„ Workerï¼ˆé˜²æ­¢æ³¨å†Œè¡¨æ— é™å¢é•¿ï¼‰
        now = time.time()
        stale = [wid for wid, info in _worker_registry.items()
                 if now - info["last_seen"] > 600]
        for wid in stale:
            del _worker_registry[wid]
            # åŒæ­¥æ¸…ç†åè°ƒå™¨ä¸­çš„è¿‡æœŸæ•°æ®
            _global_coordinator["worker_metrics"].pop(wid, None)
            _global_coordinator["worker_quotas"].pop(wid, None)
            _global_coordinator["recovery_jitter"].pop(wid, None)
        if stale:
            logger.info(f"æ¸…ç†äº† {len(stale)} ä¸ªé•¿æœŸç¦»çº¿ Worker")

        # å…œåº•é…é¢é‡ç®—ï¼ˆç¡®ä¿ Worker ç¦»çº¿åé…é¢è¢«å›æ”¶ï¼‰
        _allocate_quotas()

        await asyncio.sleep(60)


# ==================== API ç«¯ç‚¹ ====================

# --- ä»»åŠ¡ä¸Šä¼  ---
@app.post("/api/upload")
async def upload_asin_file(
    file: UploadFile = File(...),
    batch_name: str = Form(None),
    zip_code: str = Form(None),
    needs_screenshot: bool = Form(False),
):
    """
    ä¸Šä¼  ASIN æ–‡ä»¶ï¼ˆExcel/CSVï¼‰
    è‡ªåŠ¨è¯†åˆ«æ–‡ä»¶ç±»å‹ï¼Œæå– ASIN åˆ—
    """
    db = await get_db()
    zip_code = zip_code or _runtime_settings["zip_code"]

    # è‡ªåŠ¨ç”Ÿæˆæ‰¹æ¬¡å
    if not batch_name:
        batch_name = f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

    # æ‰¹æ¬¡åç™½åå•å‡€åŒ–ï¼šåªå…è®¸å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿ã€è¿å­—ç¬¦
    batch_name = re.sub(r'[^a-zA-Z0-9_\-]', '_', batch_name).strip('_')
    if not batch_name:
        batch_name = f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

    # è¯»å–æ–‡ä»¶å†…å®¹
    content = await file.read()
    filename = (file.filename or "").lower()

    # æ–‡ä»¶ç±»å‹ç™½åå•
    allowed_extensions = ('.xlsx', '.xls', '.csv', '.txt')
    if not any(filename.endswith(ext) for ext in allowed_extensions):
        raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼Œä»…å…è®¸: {', '.join(allowed_extensions)}")

    asins = []
    try:
        if filename.endswith(('.xlsx', '.xls')):
            # Excel æ–‡ä»¶
            wb = openpyxl.load_workbook(io.BytesIO(content), read_only=True)
            ws = wb.active
            for row in ws.iter_rows(values_only=True):
                for cell in row:
                    if cell:
                        val = str(cell).strip().upper()
                        # ASIN æ ¼å¼ï¼š10ä½å­—æ¯æ•°å­—ï¼Œä»¥ B å¼€å¤´
                        if re.match(r'^B[0-9A-Z]{9}$', val):
                            asins.append(val)
            wb.close()
        elif filename.endswith('.csv'):
            # CSV æ–‡ä»¶
            text = content.decode('utf-8-sig')
            reader = csv.reader(io.StringIO(text))
            for row in reader:
                for cell in row:
                    val = cell.strip().upper()
                    if re.match(r'^B[0-9A-Z]{9}$', val):
                        asins.append(val)
        else:
            # çº¯æ–‡æœ¬ï¼ˆæ¯è¡Œä¸€ä¸ª ASINï¼‰
            text = content.decode('utf-8-sig')
            for line in text.splitlines():
                val = line.strip().upper()
                if re.match(r'^B[0-9A-Z]{9}$', val):
                    asins.append(val)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"æ–‡ä»¶è§£æå¤±è´¥: {str(e)}")

    if not asins:
        raise HTTPException(status_code=400, detail="æ–‡ä»¶ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„ ASIN")

    # å»é‡
    asins = list(dict.fromkeys(asins))

    # åˆ›å»ºä»»åŠ¡
    inserted = await db.create_tasks(batch_name, asins, zip_code, needs_screenshot)

    return {
        "status": "ok",
        "batch_name": batch_name,
        "total_asins": len(asins),
        "inserted": inserted,
        "zip_code": zip_code,
        "needs_screenshot": needs_screenshot,
    }


# --- Worker æ‹‰å–ä»»åŠ¡ ---
@app.get("/api/tasks/pull")
async def pull_tasks(
    worker_id: str = Query(...),
    count: int = Query(10),
):
    """Worker æ‹‰å–å¾…å¤„ç†ä»»åŠ¡"""
    db = await get_db()
    _register_worker(worker_id)
    
    tasks = await db.pull_tasks(worker_id, max(1, min(count, 50)))
    
    if worker_id in _worker_registry:
        _worker_registry[worker_id]["tasks_pulled"] += len(tasks)

    return {"tasks": tasks}


# --- Worker é‡Šæ”¾ä»»åŠ¡ï¼ˆä¼˜å…ˆé‡‡é›†é˜Ÿåˆ—åˆ‡æ¢æ—¶å½’è¿˜æ—§ä»»åŠ¡ï¼‰---
@app.post("/api/tasks/release")
async def release_tasks(request: Request):
    """Worker å½’è¿˜æœªå¤„ç†çš„ä»»åŠ¡ï¼Œç«‹å³é‡ç½®ä¸º pendingï¼ˆé¿å…ç­‰ 5 åˆ†é’Ÿè¶…æ—¶ï¼‰"""
    db = await get_db()
    data = await request.json()
    task_ids = data.get("task_ids", [])
    if not task_ids:
        return {"status": "ok", "released": 0}
    count = await db.release_tasks(task_ids)
    logger.info(f"ğŸ”„ Worker å½’è¿˜äº† {count} ä¸ªä»»åŠ¡")
    return {"status": "ok", "released": count}


# --- Worker æäº¤ç»“æœ ---
@app.post("/api/tasks/result")
async def submit_result(request: Request):
    """Worker æäº¤é‡‡é›†ç»“æœ"""
    db = await get_db()
    data = await request.json()
    
    task_id = data.get("task_id")
    worker_id = data.get("worker_id", "unknown")
    success = data.get("success", False)
    result_data = data.get("result")

    _register_worker(worker_id)
    if worker_id in _worker_registry:
        _worker_registry[worker_id]["results_submitted"] += 1

    if success and result_data:
        # ä¿å­˜ç»“æœ
        await db.save_result(result_data)
        await db.mark_task_done(task_id, worker_id)
    else:
        await db.mark_task_failed(task_id, worker_id,
                                  error_type=data.get("error_type"),
                                  error_detail=data.get("error_detail"))

    return {"status": "ok"}


# --- Worker æ‰¹é‡æäº¤ç»“æœ ---
@app.post("/api/tasks/result/batch")
async def submit_result_batch(request: Request):
    """Worker æ‰¹é‡æäº¤é‡‡é›†ç»“æœï¼ˆç»Ÿä¸€æäº¤ï¼Œå‡å°‘ç£ç›˜ IOï¼‰"""
    db = await get_db()
    data = await request.json()
    results_list = data.get("results", [])

    try:
        for item in results_list:
            task_id = item.get("task_id")
            worker_id = item.get("worker_id", "unknown")
            success = item.get("success", False)
            result_data = item.get("result")

            _register_worker(worker_id)
            if worker_id in _worker_registry:
                _worker_registry[worker_id]["results_submitted"] += 1

            now = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')
            if success and result_data:
                # ä¿å­˜ç»“æœï¼ˆä¸å•ç‹¬ commitï¼‰
                fields = ["batch_name", "asin"] + [f for f in RESULT_FIELDS if f != "asin"]
                values = [result_data.get(f, "") for f in fields]
                placeholders = ",".join(["?"] * len(fields))
                field_names = ",".join(fields)
                await db._db.execute(
                    f"INSERT OR REPLACE INTO results ({field_names}) VALUES ({placeholders})",
                    values
                )
                # æ ‡è®°ä»»åŠ¡å®Œæˆ
                await db._db.execute(
                    "UPDATE tasks SET status = 'done', worker_id = ?, updated_at = ? WHERE id = ?",
                    (worker_id, now, task_id)
                )
            else:
                # æ ‡è®°ä»»åŠ¡å¤±è´¥ï¼ˆå«é”™è¯¯åˆ†ç±»ï¼‰
                error_type = item.get("error_type")
                error_detail = item.get("error_detail")
                await db._db.execute(
                    """UPDATE tasks
                       SET status = 'failed', worker_id = ?, retry_count = retry_count + 1,
                           error_type = ?, error_detail = ?, updated_at = ?
                       WHERE id = ?""",
                    (worker_id, error_type, error_detail, now, task_id)
                )

        # æ•´æ‰¹ç»Ÿä¸€ commit
        await db._db.commit()
    except Exception:
        await db._db.rollback()
        raise

    return {"status": "ok", "count": len(results_list)}


# --- è¿›åº¦æŸ¥è¯¢ ---
@app.get("/api/progress/{batch_name}")
async def get_progress(batch_name: str):
    """è·å–æŒ‡å®šæ‰¹æ¬¡çš„é‡‡é›†è¿›åº¦"""
    db = await get_db()
    progress = await db.get_progress(batch_name)
    return progress


@app.get("/api/progress")
async def get_overall_progress():
    """è·å–æ€»ä½“è¿›åº¦"""
    db = await get_db()
    progress = await db.get_progress()
    return progress


# --- æ‰¹æ¬¡åˆ—è¡¨ ---
@app.get("/api/batches")
async def get_batches():
    """è·å–æ‰€æœ‰æ‰¹æ¬¡åˆ—è¡¨"""
    db = await get_db()
    batches = await db.get_batch_list()
    return {"batches": batches}


# --- ç»“æœæŸ¥è¯¢ ---
@app.get("/api/results")
async def get_results(
    batch_name: str = Query(None),
    page: int = Query(1),
    per_page: int = Query(50),
    search: str = Query(None),
):
    """åˆ†é¡µè·å–é‡‡é›†ç»“æœ"""
    db = await get_db()
    results, total = await db.get_results(batch_name, page, per_page, search)
    return {
        "results": results,
        "total": total,
        "page": page,
        "per_page": per_page,
        "total_pages": (total + per_page - 1) // per_page,
    }


# --- æ•°æ®å¯¼å‡º ---
@app.get("/api/export/{batch_name}")
async def export_data(
    batch_name: str,
    format: str = Query("excel"),
):
    """å¯¼å‡ºé‡‡é›†æ•°æ®ï¼ˆExcel/CSVï¼‰"""
    db = await get_db()
    results = await db.get_all_results(batch_name)

    if not results:
        raise HTTPException(status_code=404, detail="è¯¥æ‰¹æ¬¡æ— æ•°æ®")

    if format == "csv":
        return _export_csv(results, batch_name)
    else:
        return _export_excel(results, batch_name)


@app.get("/api/export/{batch_name}/screenshots")
async def export_screenshots(batch_name: str):
    """æ‰¹é‡ä¸‹è½½æŸæ‰¹æ¬¡çš„æ‰€æœ‰æˆªå›¾ï¼ˆZIP æ‰“åŒ…ï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶é¿å…å†…å­˜å³°å€¼ï¼‰"""
    import tempfile
    # é˜²è·¯å¾„ç©¿è¶Šï¼šåªå…è®¸å®‰å…¨å­—ç¬¦
    safe_name = re.sub(r'[^a-zA-Z0-9_\-]', '_', batch_name)
    screenshot_dir = os.path.realpath(os.path.join(config.STATIC_DIR, "screenshots", safe_name))
    screenshots_root = os.path.realpath(os.path.join(config.STATIC_DIR, "screenshots"))
    if not screenshot_dir.startswith(screenshots_root + os.sep):
        raise HTTPException(status_code=400, detail="éæ³•æ‰¹æ¬¡å")
    if not os.path.isdir(screenshot_dir):
        raise HTTPException(status_code=404, detail="è¯¥æ‰¹æ¬¡æ— æˆªå›¾")

    png_files = [f for f in os.listdir(screenshot_dir) if f.endswith(".png")]
    if not png_files:
        raise HTTPException(status_code=404, detail="è¯¥æ‰¹æ¬¡æ— æˆªå›¾æ–‡ä»¶")

    # å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼Œé¿å…å¤§æ‰¹æ¬¡æˆªå›¾æ’‘çˆ†å†…å­˜
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".zip")
    try:
        with zipfile.ZipFile(tmp, "w", zipfile.ZIP_DEFLATED) as zf:
            for fname in sorted(png_files):
                fpath = os.path.join(screenshot_dir, fname)
                zf.write(fpath, fname)
        tmp_path = tmp.name
        tmp.close()
    except Exception:
        tmp.close()
        os.unlink(tmp.name)
        raise

    async def _stream_and_cleanup():
        try:
            with open(tmp_path, "rb") as f:
                while True:
                    chunk = f.read(65536)
                    if not chunk:
                        break
                    yield chunk
        finally:
            os.unlink(tmp_path)

    return StreamingResponse(
        _stream_and_cleanup(),
        media_type="application/zip",
        headers={"Content-Disposition": f'attachment; filename="{re.sub(r"[^a-zA-Z0-9_\\-]", "_", batch_name)}_screenshots.zip"'},
    )


def _export_excel(results: List[Dict], batch_name: str):
    """å¯¼å‡º Excel æ–‡ä»¶"""
    wb = openpyxl.Workbook()
    ws = wb.active
    ws.title = "é‡‡é›†ç»“æœ"

    # å†™è¡¨å¤´ï¼ˆä½¿ç”¨ä¸­æ–‡ï¼‰
    headers = []
    field_keys = []
    for field in RESULT_FIELDS:
        cn_name = config.HEADER_MAP.get(field, field)
        headers.append(cn_name)
        field_keys.append(field)

    ws.append(headers)

    # å†™æ•°æ®
    for row_data in results:
        row = [str(row_data.get(f, "")) for f in field_keys]
        ws.append(row)

    # ä¿å­˜åˆ°å†…å­˜
    output = io.BytesIO()
    wb.save(output)
    output.seek(0)

    filename = f"{batch_name}.xlsx"
    return StreamingResponse(
        output,
        media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        headers={"Content-Disposition": f"attachment; filename={filename}"},
    )


def _export_csv(results: List[Dict], batch_name: str):
    """å¯¼å‡º CSV æ–‡ä»¶"""
    output = io.StringIO()
    writer = csv.writer(output)

    # è¡¨å¤´
    headers = [config.HEADER_MAP.get(f, f) for f in RESULT_FIELDS]
    writer.writerow(headers)

    # æ•°æ®
    for row_data in results:
        row = [str(row_data.get(f, "")) for f in RESULT_FIELDS]
        writer.writerow(row)

    content = output.getvalue().encode('utf-8-sig')
    filename = f"{batch_name}.csv"
    return StreamingResponse(
        io.BytesIO(content),
        media_type="text/csv",
        headers={"Content-Disposition": f"attachment; filename={filename}"},
    )


# --- Worker ç›‘æ§ ---
@app.get("/api/workers")
async def get_workers():
    """è·å–åœ¨çº¿ worker åˆ—è¡¨"""
    now = time.time()
    workers = []
    for wid, info in _worker_registry.items():
        elapsed = now - info["last_seen"]
        is_online = elapsed < 60
        # åœ¨çº¿ï¼šnow - first_seenï¼›ç¦»çº¿ï¼šlast_seen - first_seenï¼ˆå†»ç»“æ—¶é•¿ï¼‰
        uptime = (now if is_online else info["last_seen"]) - info["first_seen"]
        quota = _global_coordinator["worker_quotas"].get(wid, {})
        metrics = _global_coordinator["worker_metrics"].get(wid, {})
        workers.append({
            "worker_id": wid,
            "status": "online" if is_online else "offline",
            "last_seen": datetime.fromtimestamp(info["last_seen"]).strftime("%H:%M:%S"),
            "tasks_pulled": info["tasks_pulled"],
            "results_submitted": info["results_submitted"],
            "uptime": int(uptime),
            "quota_concurrency": quota.get("concurrency"),
            "quota_qps": quota.get("qps"),
            "success_rate": metrics.get("success_rate"),
            "block_rate": metrics.get("block_rate"),
            "latency_p50": metrics.get("latency_p50"),
            "inflight": metrics.get("inflight"),
        })
    return {"workers": workers}


@app.delete("/api/workers/{worker_id}")
async def remove_worker(worker_id: str):
    """ç§»é™¤å•ä¸ªç¦»çº¿ worker"""
    if worker_id in _worker_registry:
        del _worker_registry[worker_id]
        return {"status": "ok", "removed": worker_id}
    return {"status": "not_found"}


@app.delete("/api/workers")
async def remove_offline_workers():
    """æ¸…ç†æ‰€æœ‰ç¦»çº¿ workerï¼ˆè¶…è¿‡ 60 ç§’æ— å¿ƒè·³ï¼‰"""
    now = time.time()
    offline = [wid for wid, info in _worker_registry.items() if now - info["last_seen"] >= 60]
    for wid in offline:
        del _worker_registry[wid]
    return {"status": "ok", "removed": len(offline)}


# --- å…¨å±€å¹¶å‘åè°ƒ ---
@app.post("/api/worker/sync")
async def worker_sync(request: Request):
    """
    Worker ç»¼åˆåŒæ­¥ç«¯ç‚¹ï¼ˆæ¯ 30s è°ƒç”¨ä¸€æ¬¡ï¼‰
    åŠŸèƒ½ï¼šå¿ƒè·³ + ä¸ŠæŠ¥ metrics + æ‹‰å– settings + æ¥æ”¶é…é¢
    """
    data = await request.json()
    worker_id = data.get("worker_id")
    if not worker_id:
        raise HTTPException(400, "worker_id required")

    metrics = data.get("metrics")

    _register_worker(worker_id)

    # å¤„ç† metricsï¼ˆå¯èƒ½è§¦å‘å…¨å±€å°é”ï¼‰
    if metrics:
        _handle_worker_metrics(worker_id, metrics)

    # ç¡®ä¿é…é¢æ˜¯æœ€æ–°çš„
    _allocate_quotas()

    # æ„å»ºå“åº”
    g = _global_coordinator
    quota = g["worker_quotas"].get(worker_id, {})
    now_mono = time.monotonic()
    in_cooldown = now_mono < g["global_block_until"]

    return {
        **_runtime_settings,
        "_version": _settings_version,
        "_quota": {
            "concurrency": quota.get("concurrency", _runtime_settings["max_concurrency"]),
            "qps": quota.get("qps", _runtime_settings["token_bucket_rate"]),
        },
        "_global_block": {
            "active": in_cooldown,
            "remaining_s": max(0, int(g["global_block_until"] - now_mono)) if in_cooldown else 0,
            "triggered_by": g.get("block_triggered_by") if in_cooldown else None,
            "epoch": g["recovery_epoch"],
        },
        "_recovery_jitter": g["recovery_jitter"].get(worker_id, 0.5),
    }


@app.get("/api/coordinator")
async def get_coordinator_state():
    """å…¨å±€å¹¶å‘åè°ƒçŠ¶æ€ï¼ˆå¯è§‚æµ‹æ€§ç«¯ç‚¹ï¼‰"""
    g = _global_coordinator
    now_mono = time.monotonic()
    in_cooldown = now_mono < g["global_block_until"]

    per_worker = {}
    for wid in g["worker_quotas"]:
        quota = g["worker_quotas"].get(wid, {})
        metrics = g["worker_metrics"].get(wid, {})
        per_worker[wid] = {
            "quota_concurrency": quota.get("concurrency"),
            "quota_qps": quota.get("qps"),
            "success_rate": metrics.get("success_rate"),
            "block_rate": metrics.get("block_rate"),
            "latency_p50": metrics.get("latency_p50"),
            "inflight": metrics.get("inflight"),
            "current_concurrency": metrics.get("current_concurrency"),
        }

    return {
        "global_max_concurrency": _runtime_settings["global_max_concurrency"],
        "global_max_qps": _runtime_settings["global_max_qps"],
        "allocated_concurrency": sum(
            q.get("concurrency", 0) for q in g["worker_quotas"].values()
        ),
        "allocated_qps": round(sum(
            q.get("qps", 0) for q in g["worker_quotas"].values()
        ), 2),
        "active_workers": len(g["worker_quotas"]),
        "global_block": {
            "active": in_cooldown,
            "remaining_s": max(0, int(g["global_block_until"] - now_mono)) if in_cooldown else 0,
            "block_count": g["block_count"],
            "recovery_epoch": g["recovery_epoch"],
        },
        "per_worker": per_worker,
    }


# --- è®¾ç½®ç®¡ç† ---
@app.get("/api/settings")
async def get_settings():
    """è·å–å½“å‰è¿è¡Œæ—¶è®¾ç½®ï¼ˆå«ç‰ˆæœ¬å·ï¼ŒWorker ç”¨äºå¢é‡åŒæ­¥ï¼‰"""
    return {**_runtime_settings, "_version": _settings_version}


@app.put("/api/settings")
async def update_settings(request: Request):
    """æ›´æ–°è¿è¡Œæ—¶è®¾ç½®ï¼ˆå«ç±»å‹ä¸èŒƒå›´æ ¡éªŒï¼‰"""
    global _settings_version
    data = await request.json()

    # ç±»å‹ä¸èŒƒå›´æ ¡éªŒè§„åˆ™ï¼š(type, min, max)
    _validators = {
        "zip_code":             (str,   None, None),
        "max_retries":          (int,   1,    10),
        "proxy_api_url":        (str,   None, None),
        "token_bucket_rate":    (float, 0.5,  50),
        "initial_concurrency":  (int,   1,    50),
        "min_concurrency":      (int,   1,    20),
        "max_concurrency":      (int,   2,    100),
        "session_rotate_every": (int,   50,   10000),
        "screenshot_concurrency": (int, 1,    6),
        "adjust_interval":      (int,   3,    60),
        "target_latency":       (float, 1,    30),
        "max_latency":          (float, 2,    60),
        "target_success_rate":  (float, 0.5,  1),
        "min_success_rate":     (float, 0.3,  1),
        "block_rate_threshold": (float, 0.01, 0.5),
        "cooldown_after_block": (int,   5,    120),
        "global_max_concurrency": (int,  2,    500),
        "global_max_qps":         (float, 0.5, 100),
    }

    changed = False
    errors = []
    old_values = {}  # ä¿å­˜æ—§å€¼ç”¨äºäº¤å‰æ ¡éªŒå¤±è´¥å›æ»š
    for key in _runtime_settings:
        if key not in data or data[key] == _runtime_settings[key]:
            continue
        val = data[key]
        validator = _validators.get(key)
        if not validator:
            continue

        expected_type, lo, hi = validator
        # ç±»å‹è½¬æ¢ä¸æ ¡éªŒ
        try:
            if expected_type is int:
                val = int(val)
            elif expected_type is float:
                val = float(val)
            elif expected_type is str:
                val = str(val)
        except (ValueError, TypeError):
            errors.append(f"{key}: æœŸæœ› {expected_type.__name__}ï¼Œæ”¶åˆ° {type(val).__name__}")
            continue

        # èŒƒå›´æ ¡éªŒ
        if lo is not None and val < lo:
            errors.append(f"{key}: ä¸èƒ½å°äº {lo}")
            continue
        if hi is not None and val > hi:
            errors.append(f"{key}: ä¸èƒ½å¤§äº {hi}")
            continue

        old_values[key] = _runtime_settings[key]
        _runtime_settings[key] = val
        changed = True

    if errors:
        # å•å­—æ®µæ ¡éªŒå¤±è´¥ï¼Œå›æ»šå·²å†™å…¥çš„å€¼
        for k, v in old_values.items():
            _runtime_settings[k] = v
        return JSONResponse(
            status_code=422,
            content={"status": "error", "errors": errors,
                     "settings": _runtime_settings, "_version": _settings_version}
        )

    # äº¤å‰çº¦æŸæ ¡éªŒï¼ˆæ‰€æœ‰å•å­—æ®µæ ¡éªŒé€šè¿‡åï¼‰
    cross_errors = []
    s = _runtime_settings
    if s["min_concurrency"] > s["max_concurrency"]:
        cross_errors.append(f"min_concurrency({s['min_concurrency']}) ä¸èƒ½å¤§äº max_concurrency({s['max_concurrency']})")
    if s["initial_concurrency"] > s["max_concurrency"]:
        cross_errors.append(f"initial_concurrency({s['initial_concurrency']}) ä¸èƒ½å¤§äº max_concurrency({s['max_concurrency']})")
    if s["initial_concurrency"] < s["min_concurrency"]:
        cross_errors.append(f"initial_concurrency({s['initial_concurrency']}) ä¸èƒ½å°äº min_concurrency({s['min_concurrency']})")
    if s["target_latency"] >= s["max_latency"]:
        cross_errors.append(f"target_latency({s['target_latency']}) å¿…é¡»å°äº max_latency({s['max_latency']})")
    if s["min_success_rate"] > s["target_success_rate"]:
        cross_errors.append(f"min_success_rate({s['min_success_rate']}) ä¸èƒ½å¤§äº target_success_rate({s['target_success_rate']})")
    if cross_errors:
        # äº¤å‰æ ¡éªŒå¤±è´¥ï¼Œå›æ»šæ‰€æœ‰å·²å†™å…¥çš„å€¼
        for k, v in old_values.items():
            _runtime_settings[k] = v
        return JSONResponse(
            status_code=422,
            content={"status": "error", "errors": cross_errors,
                     "settings": _runtime_settings, "_version": _settings_version}
        )

    if changed:
        _settings_version += 1
        logger.info(f"âš™ï¸ è®¾ç½®å·²æ›´æ–° (version={_settings_version})")
    return {"status": "ok", "settings": _runtime_settings, "_version": _settings_version}


# --- æ‰¹æ¬¡æ“ä½œ ---
@app.get("/api/batches/{batch_name}/errors")
async def batch_errors(batch_name: str):
    """è·å–æ‰¹æ¬¡çš„é”™è¯¯åˆ†ç±»ç»Ÿè®¡å’Œå¤±è´¥ä»»åŠ¡è¯¦æƒ…"""
    db = await get_db()
    summary = await db.get_error_summary(batch_name)
    failed_tasks = await db.get_failed_tasks(batch_name)
    return {"summary": summary, "tasks": failed_tasks}


@app.post("/api/batches/{batch_name}/retry")
async def retry_batch(batch_name: str):
    """é‡è¯•æ‰¹æ¬¡ä¸­æ‰€æœ‰å¤±è´¥çš„ä»»åŠ¡"""
    db = await get_db()
    await db.retry_all_failed(batch_name)
    return {"status": "ok"}


@app.post("/api/batches/{batch_name}/prioritize")
async def prioritize_batch(batch_name: str):
    """å°†æ‰¹æ¬¡ä¸­æ‰€æœ‰ pending ä»»åŠ¡è®¾ä¸ºé«˜ä¼˜å…ˆçº§"""
    db = await get_db()
    count = await db.prioritize_batch(batch_name, priority=10)
    logger.info(f"ğŸš€ æ‰¹æ¬¡ {batch_name} å·²è®¾ä¸ºä¼˜å…ˆé‡‡é›† ({count} ä¸ªä»»åŠ¡)")
    return {"status": "ok", "updated": count}


@app.delete("/api/batches/{batch_name}")
async def delete_batch(batch_name: str):
    """åˆ é™¤æ‰¹æ¬¡"""
    db = await get_db()
    await db.delete_batch(batch_name)
    return {"status": "ok"}


@app.delete("/api/database")
async def clear_database():
    """æ¸…ç©ºæ•°æ®åº“ä¸­æ‰€æœ‰æ•°æ®ï¼ˆtasks + resultsï¼‰"""
    db = await get_db()
    counts = await db.clear_all()
    return {"status": "ok", **counts}


# --- æˆªå›¾ä¸Šä¼  ---
@app.post("/api/tasks/screenshot")
async def upload_screenshot(
    file: UploadFile = File(...),
    batch_name: str = Form(...),
    asin: str = Form(...),
):
    """Worker ä¸Šä¼ æˆªå›¾æ–‡ä»¶ï¼Œä¿å­˜åˆ° static/screenshots/ å¹¶æ›´æ–° results è¡¨"""
    db = await get_db()

    # å‡€åŒ–è·¯å¾„ï¼Œé˜²è·¯å¾„ç©¿è¶Š
    safe_batch = re.sub(r'[^a-zA-Z0-9_\-]', '_', batch_name)
    safe_asin = re.sub(r'[^A-Z0-9]', '', asin.upper())
    screenshot_dir = os.path.join(config.STATIC_DIR, "screenshots", safe_batch)
    os.makedirs(screenshot_dir, exist_ok=True)

    filename = f"{safe_asin}.png"
    filepath = os.path.join(screenshot_dir, filename)
    content = await file.read()
    with open(filepath, "wb") as f:
        f.write(content)

    rel_path = f"/static/screenshots/{safe_batch}/{filename}"
    await db.update_screenshot_path(batch_name, asin, rel_path)

    logger.info(f"ğŸ“¸ æˆªå›¾å·²ä¿å­˜: {batch_name}/{asin} ({len(content)} bytes)")
    return {"status": "ok", "path": rel_path}


# --- Worker ä¸‹è½½åŒ… ---
_WORKER_FILES = [
    "worker.py", "config.py", "proxy.py", "session.py",
    "parser.py", "metrics.py", "adaptive.py", "models.py",
    "requirements-worker.txt",
]


@app.get("/api/worker/download")
async def download_worker(request: Request):
    """æ‰“åŒ… Worker æ‰€éœ€æ–‡ä»¶ä¸º ZIP ä¸‹è½½ï¼Œå†…å«å¯åŠ¨è„šæœ¬ï¼ˆè‡ªåŠ¨è¿æ¥æœ¬æœåŠ¡å™¨ï¼‰"""
    # æ¨æ–­æœåŠ¡å™¨åœ°å€ï¼ˆç”¨è¯·æ±‚çš„ Host å¤´ï¼‰
    host = request.headers.get("host", f"127.0.0.1:{config.SERVER_PORT}")
    scheme = "https" if request.headers.get("x-forwarded-proto") == "https" else "http"
    server_url = f"{scheme}://{host}"

    buf = io.BytesIO()
    with zipfile.ZipFile(buf, "w", zipfile.ZIP_DEFLATED) as zf:
        # Python æºç  + requirements
        for fname in _WORKER_FILES:
            fpath = os.path.join(config.BASE_DIR, fname)
            if os.path.exists(fpath):
                zf.write(fpath, f"worker/{fname}")

        # å¯é€‰æˆªå›¾ä¾èµ–
        zf.writestr("worker/requirements-screenshot.txt",
                     "# æˆªå›¾åŠŸèƒ½ï¼ˆå¯é€‰ï¼‰ï¼Œå®‰è£…åéœ€è¿è¡Œ: playwright install chromium\n"
                     "playwright>=1.40.0\n")

        # å¯åŠ¨è„šæœ¬ - macOS/Linux
        start_sh = f'''#!/bin/bash
# Amazon Scraper v2 - Worker å¯åŠ¨è„šæœ¬
# æœåŠ¡å™¨åœ°å€: {server_url}

set -e
cd "$(dirname "$0")"
SERVER="{server_url}"

# æ£€æµ‹ Python
if command -v python3 &>/dev/null; then
    PY=python3
elif command -v python &>/dev/null; then
    PY=python
else
    echo "é”™è¯¯: æœªæ‰¾åˆ° Pythonï¼Œè¯·å…ˆå®‰è£… Python 3.10+"
    exit 1
fi

echo "ä½¿ç”¨ Python: $($PY --version)"

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆé¦–æ¬¡è¿è¡Œï¼‰
if [ ! -d ".venv" ]; then
    echo "åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ..."
    $PY -m venv .venv
fi

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source .venv/bin/activate

# å®‰è£…ä¾èµ–ï¼ˆé¦–æ¬¡è¿è¡Œæˆ–ä¾èµ–æ›´æ–°ï¼‰
if [ ! -f ".deps_installed" ] || [ "requirements-worker.txt" -nt ".deps_installed" ]; then
    echo "å®‰è£…æ ¸å¿ƒä¾èµ–..."
    pip install -q -r requirements-worker.txt
    echo "å®‰è£…æˆªå›¾ç»„ä»¶ (playwright)..."
    pip install -q -r requirements-screenshot.txt
    playwright install chromium
    touch .deps_installed
fi

echo ""
echo "========================================="
echo "  Amazon Scraper v2 - Worker"
echo "  æœåŠ¡å™¨: $SERVER"
echo "========================================="
echo ""

python worker.py --server "$SERVER" "$@"
'''
        zf.writestr("worker/start.sh", start_sh)

        # å¯åŠ¨è„šæœ¬ - Windows
        start_bat = f'''@echo off
chcp 65001 >nul
title Amazon Scraper v2 - Worker
cd /d "%~dp0"
set SERVER={server_url}

where python >nul 2>&1
if errorlevel 1 (
    echo é”™è¯¯: æœªæ‰¾åˆ° Pythonï¼Œè¯·å…ˆå®‰è£… Python 3.10+
    pause
    exit /b 1
)

if not exist ".venv" (
    echo åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ...
    python -m venv .venv
)

call .venv\\Scripts\\activate.bat

if not exist ".deps_installed" (
    echo å®‰è£…æ ¸å¿ƒä¾èµ–...
    pip install -q -r requirements-worker.txt
    echo å®‰è£…æˆªå›¾ç»„ä»¶ (playwright^)...
    pip install -q -r requirements-screenshot.txt
    playwright install chromium
    echo. > .deps_installed
)

echo.
echo =========================================
echo   Amazon Scraper v2 - Worker
echo   æœåŠ¡å™¨: %SERVER%
echo =========================================
echo.

python worker.py --server "%SERVER%" %*
pause
'''
        zf.writestr("worker/start.bat", start_bat)

    buf.seek(0)
    return StreamingResponse(
        buf,
        media_type="application/zip",
        headers={"Content-Disposition": "attachment; filename=worker.zip"},
    )


# ==================== Web UI è·¯ç”± ====================

@app.get("/", response_class=HTMLResponse)
async def dashboard(request: Request):
    """ä»ªè¡¨ç›˜é¦–é¡µ"""
    db = await get_db()
    progress = await db.get_progress()
    batches = await db.get_batch_list()
    
    # è®¡ç®—é€Ÿåº¦ï¼ˆæœ€è¿‘5åˆ†é’Ÿçš„å®Œæˆæ•°ï¼‰
    now = time.time()
    active_workers = sum(1 for w in _worker_registry.values() if now - w["last_seen"] < 60)

    return templates.TemplateResponse("dashboard.html", {
        "request": request,
        "progress": progress,
        "batches": batches[:5],  # æœ€è¿‘5ä¸ªæ‰¹æ¬¡
        "active_workers": active_workers,
        "total_workers": len(_worker_registry),
    })


@app.get("/tasks", response_class=HTMLResponse)
async def tasks_page(request: Request):
    """ä»»åŠ¡ç®¡ç†é¡µé¢"""
    db = await get_db()
    batches = await db.get_batch_list()
    return templates.TemplateResponse("tasks.html", {
        "request": request,
        "batches": batches,
    })


@app.get("/results", response_class=HTMLResponse)
async def results_page(
    request: Request,
    batch_name: str = None,
    page: int = 1,
    search: str = None,
):
    """ç»“æœæµè§ˆé¡µé¢"""
    db = await get_db()
    batches = await db.get_batch_list()
    results, total = await db.get_results(batch_name, page, 50, search)
    total_pages = (total + 49) // 50
    progress = await db.get_progress(batch_name)

    return templates.TemplateResponse("results.html", {
        "request": request,
        "results": results,
        "batches": batches,
        "current_batch": batch_name,
        "current_page": page,
        "total": total,
        "total_pages": total_pages,
        "search": search or "",
        "progress": progress,
    })


@app.get("/settings", response_class=HTMLResponse)
async def settings_page(request: Request):
    """è®¾ç½®é¡µé¢"""
    return templates.TemplateResponse("settings.html", {
        "request": request,
        "settings": _runtime_settings,
        "config": {
            "port": config.SERVER_PORT,
            "impersonate": config.IMPERSONATE_BROWSER,
            "timeout": config.REQUEST_TIMEOUT,
        },
    })


@app.get("/workers", response_class=HTMLResponse)
async def workers_page(request: Request):
    """Worker ç›‘æ§é¡µé¢"""
    now = time.time()
    workers = []
    for wid, info in _worker_registry.items():
        elapsed = now - info["last_seen"]
        is_online = elapsed < 60
        uptime = (now if is_online else info["last_seen"]) - info["first_seen"]
        workers.append({
            "worker_id": wid,
            "status": "online" if is_online else "offline",
            "last_seen": datetime.fromtimestamp(info["last_seen"]).strftime("%Y-%m-%d %H:%M:%S"),
            "tasks_pulled": info["tasks_pulled"],
            "results_submitted": info["results_submitted"],
            "uptime_min": int(uptime / 60),
        })
    return templates.TemplateResponse("workers.html", {
        "request": request,
        "workers": workers,
    })


# ==================== ä¸»å…¥å£ ====================

if __name__ == "__main__":
    uvicorn.run(
        "server:app",
        host=config.SERVER_HOST,
        port=config.SERVER_PORT,
        reload=False,
        log_level="info",
    )
